{"meta":{"title":"一护的Blog","subtitle":null,"description":null,"author":"bleach一护","url":"http://yoursite.com","root":"/"},"pages":[{"title":"categories","date":"2019-03-09T03:21:55.000Z","updated":"2019-03-09T03:21:55.192Z","comments":true,"path":"categories/index.html","permalink":"http://yoursite.com/categories/index.html","excerpt":"","text":""},{"title":"about","date":"2019-03-07T15:37:41.000Z","updated":"2019-03-07T15:39:04.293Z","comments":false,"path":"about/index.html","permalink":"http://yoursite.com/about/index.html","excerpt":"","text":""},{"title":"tags","date":"2019-03-09T03:21:41.000Z","updated":"2019-03-09T03:21:41.386Z","comments":true,"path":"tags/index.html","permalink":"http://yoursite.com/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"正则表达式学习1","slug":"正则表达式1","date":"2019-02-05T16:00:00.000Z","updated":"2019-03-16T15:12:28.221Z","comments":true,"path":"2019/02/06/正则表达式1/","link":"","permalink":"http://yoursite.com/2019/02/06/正则表达式1/","excerpt":"","text":"正则表达式的初步学习第一次接触正则是在大二的时候，现在长时间不使用，有些概念比较模糊，因此准备重拾一下自己的知识。 正则表达式是什么？ 正则表达式在菜鸟教程中的概念如下： 正则表达式(regular expression)描述了一种字符串匹配的模式（pattern），可以用来检查一个串是否含有某种子串、将匹配的子串替换或者从某个串中取出符合某个条件的子串等 我的个人理解就是来对文本进行匹配的。 正则表达式的基础规则： 位置标记： 位置标记锚点是表识字符串位置的正则表达式。| 正则表达式 | 描述 | 实例||——–|——–|——–|| ^ | 指定了匹配正则表达式的文本必须始于字符串的首部|^zxc可匹配以zxc为首的行|| $ | 指定了匹配正则表达式的文本必须始于字符串的尾部 | zxc$可匹配以zxc结尾的行| 标识符: 标识符是正则表达式的基础部分 | 正则表达式 | 描述 |实例||——–|——–|——–|| . | 匹配任意一个字符 | asq.可以匹配asq1或者asqe 但是不能匹配asqer 其只能匹配单个的字符||[ ] |匹配括号中的任意一个字符，中括号内可以为字符组或者字符范围|par[kj]可以匹配park或者parj ,[0-9]可以匹配任意的单个数字||[ ^ ]| 匹配不在中括号中的任意一个字符| 这个就比较好理解如 9[^12]就不能匹配91 或者 92| 数量修饰符: 数量修饰符定义了模式可出现的次数| 正则表达式 | 描述 |实例||——–|——–|——–|| ? | 只能够匹配之前的项1次或者0次 | courl?可以匹配courl 或者是cour ||+| 匹配之前的一项1次或者多次 |courl+ 可以匹配courl或者 courll 等等|| | 可以匹配之前的一项0次或者多次 | c ourl可以匹配 courl或者ccourl或者是ourl 等等 ||{n}| 匹配之前的项n次 |如[0-9]{2} 能够匹配任意的两位数||{n,}|之前的项至少要匹配n次|如[0-9]{4}能够匹配任意一个4位数或者更多位的数字||{n,m}|之前的项匹配的最小次数与最多次数|[0-9]{2,5}匹配两位数到五位数之前的任何数字| 正则表达式的基本规则大概就是这些，之后会给出如何匹配出你想要得到的数据","categories":[{"name":"Hexo","slug":"Hexo","permalink":"http://yoursite.com/categories/Hexo/"},{"name":"正则表达式学习","slug":"Hexo/正则表达式学习","permalink":"http://yoursite.com/categories/Hexo/正则表达式学习/"}],"tags":[{"name":"Hexo","slug":"Hexo","permalink":"http://yoursite.com/tags/Hexo/"},{"name":"正则表达式学习","slug":"正则表达式学习","permalink":"http://yoursite.com/tags/正则表达式学习/"}]},{"title":"xpath中常用语法","slug":"xpath常用语法总结","date":"2019-02-03T16:00:00.000Z","updated":"2019-03-14T04:32:21.673Z","comments":true,"path":"2019/02/04/xpath常用语法总结/","link":"","permalink":"http://yoursite.com/2019/02/04/xpath常用语法总结/","excerpt":"","text":"xpath常用语法 xpath是一门在XML文档中查找信息的语言，在爬虫中常用来查找信息，因此掌握其语法，可以帮助我们很好的去查找想要获得的信息。 如何来进行节点的选择 选择一个适当的节点，对于找寻信息至关重要。往往一个/或者//的选取会得到不同的信息，而且可能和你想像的相差甚远。接下来，就来说明几个重要的概念。 父节点与子节点: 如下图所示:的解释一样。 下面来具体的讲讲如何来选取节点: xpath使用路径表达式在XML文档中选择节点的，节点是沿着路径或者step来选取的. | 表达式 | 描述 || —— | —— || / | 从根节点来选取 || // | 从当前节点进行选取，不考虑其位置 || @ | 选取属性 || .| 选取当前节点 | 以一个例子来实际操作一下:先选取其父节点，然后依次来找出信息所在位置的节点信息，最后获得其内容当然，一般浏览器都可以自动获得xpath，但是还是有必要学习一下，以此为例子，我们能够看出来实际中如何取操作这些节点来得到我们想要得到的消息. 最后来推荐一个chrome插件，来帮助大家来学习xpath，叫做xpath helper 前提是你可以科学上网","categories":[{"name":"Hexo","slug":"Hexo","permalink":"http://yoursite.com/categories/Hexo/"},{"name":"python爬虫","slug":"Hexo/python爬虫","permalink":"http://yoursite.com/categories/Hexo/python爬虫/"}],"tags":[{"name":"Hexo","slug":"Hexo","permalink":"http://yoursite.com/tags/Hexo/"},{"name":"python爬虫","slug":"python爬虫","permalink":"http://yoursite.com/tags/python爬虫/"}]},{"title":"用lxml与requests模块爬取起点中文网的免费阅读列表","slug":"用python中的lxml模块爬取起点网免费阅读列表","date":"2019-02-02T16:00:00.000Z","updated":"2019-03-10T03:35:57.332Z","comments":true,"path":"2019/02/03/用python中的lxml模块爬取起点网免费阅读列表/","link":"","permalink":"http://yoursite.com/2019/02/03/用python中的lxml模块爬取起点网免费阅读列表/","excerpt":"","text":"用lxml与requests模块爬取起点中文网的免费阅读列表爬取的思路: 首先来讲,爬取内容并非是最重要的,我个人认为要先知道思路,然后再去写代码,就会容易很多。 再就来说说，要写这个爬虫的思路: 首先，应该先看看网站的robots.txt文件，来看一下网站上什么内容可以，让我们去爬取,这里附上其内容，可以看到起点是应该全都可让我们去爬的，然后放心的去写代码。 1234 User-Agent: *Allow: /Allow: /*.cssAllow: /*.js 接下来，就是要获得其URL资源，注意到起点网上的free区的URL是如下的，然后我们尝试着去删除一些无用的信息，来简化URL,来获得同样的页面。https://www.qidian.com/free/all?orderId=&amp;vip=hidden&amp;style=1&amp;pageSize=20&amp;siteid=1&amp;pubflag=0&amp;hiddenField=1&amp;page=2通过尝试我们只要留下 all?page=2 这一部分就可以获得我们想要的结果。于是修改URL为https://www.qidian.com/free/all?page={}这样可以通过format() 方法来构造我们要爬取多少页。 下一步，就要去构造xpath，来获得数据,在chrome浏览器中按F12来查看网页的源码，接着我们就要去找到信息的位置: 12&lt;div class=&quot;book-mid-info&quot;&gt; &lt;h4&gt;&lt;a href=&quot;//book.qidian.com/info/1010741811&quot; target=&quot;_blank&quot; data-eid=&quot;qd_E05&quot; data-bid=&quot;1010741811&quot;&gt;大劫主&lt;/a&gt;&lt;/h4&gt; 对于此xml的节点，我们要得到一个book的名字就只要用etree下的xpath方法，就可以得到想要的信息。代码如下：xpath(&quot;//div[@class=&#39;book-img-text&#39;]//h4//a/text()&quot;)这样就可以得到我们想要的数据了。但是有的地方可能有”\\r”或者是空格只要用replace()方法替代一下就可以了。 最后附上全部的代码：附:爬取的结果图：这样就可以基本上爬到想要的信息了123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354import requestsimport jsonfrom lxml import etreeclass qidian: def __init__(self): self.headers =&#123;&quot;User-Agent&quot;: &quot;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/66.0.3359.139 Safari/537.36&quot;&#125; self.start_url=&quot;https://www.qidian.com/free/all?page=&#123;&#125;&quot; def get_free_url_List(self): return [self.start_url.format(i) for i in range(1,120)] def parse_url(self,url): response=requests.get(url,headers=self.headers) return response.content def get_url_list(self,html_url): html=etree.HTML(html_url) div_lists=html.xpath(&quot;//div[@class=&apos;all-book-list&apos;]&quot;) content_list=[] for div in div_lists: item=&#123;&#125; item[&quot;title&quot;]=div.xpath(&quot;.//h4/a/text()&quot;) item[&quot;title&quot;]=item[&quot;title&quot;][0] if len(item[&quot;title&quot;][0])&gt;0 else None item[&quot;author&quot;]=div.xpath(&quot;.//p[@class=&apos;author&apos;]//a[@class=&apos;name&apos;]/text()&quot;) item[&quot;author&quot;]=item[&quot;author&quot;][0] if len(item[&quot;author&quot;][0])&gt;0 else None item[&quot;content-sum&quot;]=div.xpath(&quot;.//p[@class=&apos;intro&apos;]/text()&quot;) item[&quot;content-sum&quot;]=[div.replace(&quot;\\r&quot;,&quot;&quot;) for div in item[&quot;content-sum&quot;]] item[&quot;content-sum&quot;]=[div.replace(&quot; &quot;,&quot;&quot;) for div in item[&quot;content-sum&quot;]] item[&quot;href&quot;]=&quot;https:&quot;+div.xpath(&quot;.//h4/a/@href&quot;)[0] if len(div.xpath(&quot;.//h4/a/@href&quot;)[0])&gt;0 else None item[&quot;img&quot;]=&quot;https:&quot;+div.xpath(&quot;.//div[@class=&apos;book-img-box&apos;]//img/@src&quot;)[0] if len(div.xpath(&quot;.//div[@class=&apos;book-img-box&apos;]//img/@src&quot;)[0])&gt;0 else None content_list.append(item) return content_list def save_content(self,content_list): with open(&quot;起点中文网免费文章1.txt&quot;,&quot;a&quot;,encoding=&apos;utf-8&apos; ) as f: for i in content_list: f.write(json.dumps(i, ensure_ascii=False)) f.write(&quot;\\n&quot;) # 写入换行符，进行换行 print(&quot;保存成功！&quot;) def run(self): # 发送请求，获得响应 # 接收数据，并处理数据 # 保存数据 url_list=self.get_free_url_List() for url in url_list: html_url=self.parse_url(url) content_list=self.get_url_list(html_url) self.save_content(content_list)if __name__ == &quot;__main__&quot;: spider_qidian=qidian() spider_qidian.run()","categories":[{"name":"Hexo","slug":"Hexo","permalink":"http://yoursite.com/categories/Hexo/"},{"name":"python爬虫","slug":"Hexo/python爬虫","permalink":"http://yoursite.com/categories/Hexo/python爬虫/"}],"tags":[{"name":"Hexo","slug":"Hexo","permalink":"http://yoursite.com/tags/Hexo/"},{"name":"python爬虫","slug":"python爬虫","permalink":"http://yoursite.com/tags/python爬虫/"}]},{"title":"python导入lxml中的etree模块","slug":"lxml模块的下载","date":"2019-01-31T16:00:00.000Z","updated":"2019-03-09T03:52:46.805Z","comments":true,"path":"2019/02/01/lxml模块的下载/","link":"","permalink":"http://yoursite.com/2019/02/01/lxml模块的下载/","excerpt":"","text":"python导入lxml中的etree模块 首先，使用pycharm中自带的下载器，下载的是不带etree模块的 再一点就是，看网上说部分lxml版本是不含etree模块的应该去找能和自己python版本兼容而且有etree模块的lxml版本 可以去官网上下载自己想要的版本官网可用版本链接 官网 如何去安装lxml模块 下载好这样一个文件:lxml-3.7.2-cp36-cp36m-win32.whl 后在cmd中用 pip install lxml-3.7.2-cp36-cp36m-win32.whl 看提示安装完成后，在cmd中测试查看是否可以使用etree模块,就是进入python,导入etree模块:from lxml import etree,若不报错，说明成功，否则就要检查是哪里出现了问题 注意命名文件时不要和这些模块重名，这样也会导入不成功","categories":[{"name":"Hexo","slug":"Hexo","permalink":"http://yoursite.com/categories/Hexo/"},{"name":"python爬虫","slug":"Hexo/python爬虫","permalink":"http://yoursite.com/categories/Hexo/python爬虫/"}],"tags":[{"name":"Hexo","slug":"Hexo","permalink":"http://yoursite.com/tags/Hexo/"},{"name":"python爬虫","slug":"python爬虫","permalink":"http://yoursite.com/tags/python爬虫/"}]},{"title":"Hello World","slug":"hello-world","date":"2018-09-30T16:00:00.000Z","updated":"2019-03-09T03:53:34.973Z","comments":true,"path":"2018/10/01/hello-world/","link":"","permalink":"http://yoursite.com/2018/10/01/hello-world/","excerpt":"","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new \"My New Post\" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","categories":[{"name":"Hexo","slug":"Hexo","permalink":"http://yoursite.com/categories/Hexo/"}],"tags":[{"name":"Hexo","slug":"Hexo","permalink":"http://yoursite.com/tags/Hexo/"}]}]}