{"meta":{"title":"一护的Blog","subtitle":null,"description":null,"author":"bleach一护","url":"http://yoursite.com","root":"/"},"pages":[{"title":"tags","date":"2019-03-09T03:21:41.000Z","updated":"2019-03-09T03:21:41.386Z","comments":true,"path":"tags/index.html","permalink":"http://yoursite.com/tags/index.html","excerpt":"","text":""},{"title":"categories","date":"2019-03-09T03:21:55.000Z","updated":"2019-03-09T03:21:55.192Z","comments":true,"path":"categories/index.html","permalink":"http://yoursite.com/categories/index.html","excerpt":"","text":""},{"title":"about","date":"2019-03-07T15:37:41.000Z","updated":"2019-03-07T15:39:04.293Z","comments":false,"path":"about/index.html","permalink":"http://yoursite.com/about/index.html","excerpt":"","text":""}],"posts":[{"title":"160个crackme02","slug":"160个crackme02","date":"2019-04-10T16:00:00.000Z","updated":"2019-09-17T13:02:07.551Z","comments":true,"path":"2019/04/11/160个crackme02/","link":"","permalink":"http://yoursite.com/2019/04/11/160个crackme02/","excerpt":"","text":"160个crackme02 1.首先来尝试爆破 首先直接打开这个小程序随便输入一个账号和密码，来尝试一下是什么效果：我们通过输入可以看出来会有弹窗，这样我们就可以用OD的插件来去搜索字符串，找到地址来具体分析，之后我们打开OD来用插件查看其地址. 我们进入去分析，我们看到了前面有一个je，并且跳转地址是判断错误的地方，断定这个地方就是关键跳转点。,于是我们就可以通过把ZF的值改变，来爆破。也可以nop填充来爆破。当然这只是第一步，下面就来对密码生成的算法来尝试破解 2.破解密码生成的算法 我们这里就要从头开始分析，第一个函数，是来获得你输入账号的长度。再用其长度0x17CFB.第二个函数，是获得你输入账号的第一个字符的ascii码的值，然后，再把值和上一步得到的结果相加。后面的汇编很长，在这里还有一个是把“AKA-”这个前缀给了密码。但是都不是检查输入的密码和生成密码是否相同的，我们要去找到一个比较两者是否相同的函数.从这个地方看到一个cmp的函数推测是来比较密码是否一致的，我们下一个断点，F7跟进一下。我们可以看到在函数中就有了密码的比较，在这个栈中可以清楚的看到两个密码，于是可以断定密码的生成方法就是: **AKA-长度0x17CFB+账号的第一个字符的ascii码的值**然后写一个简单python脚本来实现此过程: 12345str1=input(&quot;输入一个字符串&quot;)pass1 =len(str1)*0x17CFBpass2=ord(str1[0])+pass1pass3=&quot;AKA-&quot;+str(pass2)print(pass3) 然后我们来测试一下是否正确:到此为止就已经完成了crackme02！！！","categories":[{"name":"Hexo","slug":"Hexo","permalink":"http://yoursite.com/categories/Hexo/"}],"tags":[{"name":"Hexo","slug":"Hexo","permalink":"http://yoursite.com/tags/Hexo/"},{"name":"破解基础","slug":"破解基础","permalink":"http://yoursite.com/tags/破解基础/"}]},{"title":"正则表达式学习1","slug":"正则表达式1","date":"2019-02-05T16:00:00.000Z","updated":"2019-03-16T15:40:19.730Z","comments":true,"path":"2019/02/06/正则表达式1/","link":"","permalink":"http://yoursite.com/2019/02/06/正则表达式1/","excerpt":"","text":"正则表达式的初步学习第一次接触正则是在大二的时候，现在长时间不使用，有些概念比较模糊，因此准备重拾一下自己的知识。 正则表达式是什么？ 正则表达式在菜鸟教程中的概念如下： 正则表达式(regular expression)描述了一种字符串匹配的模式（pattern），可以用来检查一个串是否含有某种子串、将匹配的子串替换或者从某个串中取出符合某个条件的子串等 我的个人理解就是来对文本进行匹配的。 正则表达式的基础规则： 位置标记： 位置标记锚点是表识字符串位置的正则表达式。 正则表达式 描述 实例 ^ 指定了匹配正则表达式的文本必须始于字符串的首部 ^zxc可匹配以zxc为首的行 $ 指定了匹配正则表达式的文本必须始于字符串的尾部 zxc$可匹配以zxc结尾的行 标识符: 标识符是正则表达式的基础部分 正则表达式 描述 实例 . 匹配任意一个字符 asq.可以匹配asq1或者asqe 但是不能匹配asqer 其只能匹配单个的字符 [ ] 匹配括号中的任意一个字符，中括号内可以为字符组或者字符范围 par[kj]可以匹配park或者parj ,[0-9]可以匹配任意的单个数字 [ ^ ] 匹配不在中括号中的任意一个字符 这个就比较好理解如 9[^12]就不能匹配91 或者 92 数量修饰符: 数量修饰符定义了模式可出现的次数 正则表达式 描述 实例 ? 只能够匹配之前的项1次或者0次 courl?可以匹配courl 或者是cour + 匹配之前的一项1次或者多次 courl+ 可以匹配courl或者 courll 等等 * 可以匹配之前的一项0次或者多次 c * ourl可以匹配 courl或者ccourl或者是ourl 等等 {n} 匹配之前的项n次 如[0-9]{2} 能够匹配任意的两位数 {n,} 之前的项至少要匹配n次 如[0-9]{4}能够匹配任意一个4位数或者更多位的数字 {n,m} 之前的项匹配的最小次数与最多次数 [0-9]{2,5}匹配两位数到五位数之前的任何数字 正则表达式的基本规则大概就是这些，之后会给出如何匹配出你想要得到的数据","categories":[{"name":"Hexo","slug":"Hexo","permalink":"http://yoursite.com/categories/Hexo/"},{"name":"正则表达式学习","slug":"Hexo/正则表达式学习","permalink":"http://yoursite.com/categories/Hexo/正则表达式学习/"}],"tags":[{"name":"Hexo","slug":"Hexo","permalink":"http://yoursite.com/tags/Hexo/"},{"name":"正则表达式学习","slug":"正则表达式学习","permalink":"http://yoursite.com/tags/正则表达式学习/"}]},{"title":"xpath中常用语法","slug":"xpath常用语法总结","date":"2019-02-03T16:00:00.000Z","updated":"2019-04-09T14:50:06.675Z","comments":true,"path":"2019/02/04/xpath常用语法总结/","link":"","permalink":"http://yoursite.com/2019/02/04/xpath常用语法总结/","excerpt":"","text":"xpath常用语法 xpath是一门在XML文档中查—title: 正则表达式学习1 date: 2019-02-06tags: Hexo 正则表达式学习categories: Hexo 正则表达式学习 —找信息的语言，在爬虫中常用来查找信息，因此掌握其语法，可以帮助我们很好的去查找想要获得的信息。 如何来进行节点的选择 选择一个适当的节点，对于找寻信息至关重要。往往一个/或者//的选取会得到不同的信息，而且可能和你想像的相差甚远。接下来，就来说明几个重要的概念。 父节点与子节点: 如下图所示:的解释一样。 下面来具体的讲讲如何来选取节点: xpath使用路径表达式在XML文档中选择节点的，节点是沿着路径或者step来选取的. 表达式 描述 / 从根节点来选取 // 从当前节点进行选取，不考虑其位置 @ 选取属性 . 选取当前节点 以一个例子来实际操作一下:先选取其父节点，然后依次来找出信息所在位置的节点信息，最后获得其内容当然，一般浏览器都可以自动获得xpath，但是还是有必要学习一下，以此为例子，我们能够看出来实际中如何取操作这些节点来得到我们想要得到的消息. 最后来推荐一个chrome插件，来帮助大家来学习xpath，叫做xpath helper 前提是你可以科学上网","categories":[{"name":"Hexo","slug":"Hexo","permalink":"http://yoursite.com/categories/Hexo/"},{"name":"python爬虫","slug":"Hexo/python爬虫","permalink":"http://yoursite.com/categories/Hexo/python爬虫/"}],"tags":[{"name":"Hexo","slug":"Hexo","permalink":"http://yoursite.com/tags/Hexo/"},{"name":"python爬虫","slug":"python爬虫","permalink":"http://yoursite.com/tags/python爬虫/"}]},{"title":"用lxml与requests模块爬取起点中文网的免费阅读列表","slug":"用python中的lxml模块爬取起点网免费阅读列表","date":"2019-02-02T16:00:00.000Z","updated":"2019-03-10T03:35:57.332Z","comments":true,"path":"2019/02/03/用python中的lxml模块爬取起点网免费阅读列表/","link":"","permalink":"http://yoursite.com/2019/02/03/用python中的lxml模块爬取起点网免费阅读列表/","excerpt":"","text":"用lxml与requests模块爬取起点中文网的免费阅读列表爬取的思路: 首先来讲,爬取内容并非是最重要的,我个人认为要先知道思路,然后再去写代码,就会容易很多。 再就来说说，要写这个爬虫的思路: 首先，应该先看看网站的robots.txt文件，来看一下网站上什么内容可以，让我们去爬取,这里附上其内容，可以看到起点是应该全都可让我们去爬的，然后放心的去写代码。 1234 User-Agent: *Allow: /Allow: /*.cssAllow: /*.js 接下来，就是要获得其URL资源，注意到起点网上的free区的URL是如下的，然后我们尝试着去删除一些无用的信息，来简化URL,来获得同样的页面。https://www.qidian.com/free/all?orderId=&amp;vip=hidden&amp;style=1&amp;pageSize=20&amp;siteid=1&amp;pubflag=0&amp;hiddenField=1&amp;page=2通过尝试我们只要留下 all?page=2 这一部分就可以获得我们想要的结果。于是修改URL为https://www.qidian.com/free/all?page={}这样可以通过format() 方法来构造我们要爬取多少页。 下一步，就要去构造xpath，来获得数据,在chrome浏览器中按F12来查看网页的源码，接着我们就要去找到信息的位置: 12&lt;div class=&quot;book-mid-info&quot;&gt; &lt;h4&gt;&lt;a href=&quot;//book.qidian.com/info/1010741811&quot; target=&quot;_blank&quot; data-eid=&quot;qd_E05&quot; data-bid=&quot;1010741811&quot;&gt;大劫主&lt;/a&gt;&lt;/h4&gt; 对于此xml的节点，我们要得到一个book的名字就只要用etree下的xpath方法，就可以得到想要的信息。代码如下：xpath(&quot;//div[@class=&#39;book-img-text&#39;]//h4//a/text()&quot;)这样就可以得到我们想要的数据了。但是有的地方可能有”\\r”或者是空格只要用replace()方法替代一下就可以了。 最后附上全部的代码：附:爬取的结果图：这样就可以基本上爬到想要的信息了123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354import requestsimport jsonfrom lxml import etreeclass qidian: def __init__(self): self.headers =&#123;&quot;User-Agent&quot;: &quot;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/66.0.3359.139 Safari/537.36&quot;&#125; self.start_url=&quot;https://www.qidian.com/free/all?page=&#123;&#125;&quot; def get_free_url_List(self): return [self.start_url.format(i) for i in range(1,120)] def parse_url(self,url): response=requests.get(url,headers=self.headers) return response.content def get_url_list(self,html_url): html=etree.HTML(html_url) div_lists=html.xpath(&quot;//div[@class=&apos;all-book-list&apos;]&quot;) content_list=[] for div in div_lists: item=&#123;&#125; item[&quot;title&quot;]=div.xpath(&quot;.//h4/a/text()&quot;) item[&quot;title&quot;]=item[&quot;title&quot;][0] if len(item[&quot;title&quot;][0])&gt;0 else None item[&quot;author&quot;]=div.xpath(&quot;.//p[@class=&apos;author&apos;]//a[@class=&apos;name&apos;]/text()&quot;) item[&quot;author&quot;]=item[&quot;author&quot;][0] if len(item[&quot;author&quot;][0])&gt;0 else None item[&quot;content-sum&quot;]=div.xpath(&quot;.//p[@class=&apos;intro&apos;]/text()&quot;) item[&quot;content-sum&quot;]=[div.replace(&quot;\\r&quot;,&quot;&quot;) for div in item[&quot;content-sum&quot;]] item[&quot;content-sum&quot;]=[div.replace(&quot; &quot;,&quot;&quot;) for div in item[&quot;content-sum&quot;]] item[&quot;href&quot;]=&quot;https:&quot;+div.xpath(&quot;.//h4/a/@href&quot;)[0] if len(div.xpath(&quot;.//h4/a/@href&quot;)[0])&gt;0 else None item[&quot;img&quot;]=&quot;https:&quot;+div.xpath(&quot;.//div[@class=&apos;book-img-box&apos;]//img/@src&quot;)[0] if len(div.xpath(&quot;.//div[@class=&apos;book-img-box&apos;]//img/@src&quot;)[0])&gt;0 else None content_list.append(item) return content_list def save_content(self,content_list): with open(&quot;起点中文网免费文章1.txt&quot;,&quot;a&quot;,encoding=&apos;utf-8&apos; ) as f: for i in content_list: f.write(json.dumps(i, ensure_ascii=False)) f.write(&quot;\\n&quot;) # 写入换行符，进行换行 print(&quot;保存成功！&quot;) def run(self): # 发送请求，获得响应 # 接收数据，并处理数据 # 保存数据 url_list=self.get_free_url_List() for url in url_list: html_url=self.parse_url(url) content_list=self.get_url_list(html_url) self.save_content(content_list)if __name__ == &quot;__main__&quot;: spider_qidian=qidian() spider_qidian.run()","categories":[{"name":"Hexo","slug":"Hexo","permalink":"http://yoursite.com/categories/Hexo/"},{"name":"python爬虫","slug":"Hexo/python爬虫","permalink":"http://yoursite.com/categories/Hexo/python爬虫/"}],"tags":[{"name":"Hexo","slug":"Hexo","permalink":"http://yoursite.com/tags/Hexo/"},{"name":"python爬虫","slug":"python爬虫","permalink":"http://yoursite.com/tags/python爬虫/"}]},{"title":"python导入lxml中的etree模块","slug":"lxml模块的下载","date":"2019-01-31T16:00:00.000Z","updated":"2019-03-09T03:52:46.805Z","comments":true,"path":"2019/02/01/lxml模块的下载/","link":"","permalink":"http://yoursite.com/2019/02/01/lxml模块的下载/","excerpt":"","text":"python导入lxml中的etree模块 首先，使用pycharm中自带的下载器，下载的是不带etree模块的 再一点就是，看网上说部分lxml版本是不含etree模块的应该去找能和自己python版本兼容而且有etree模块的lxml版本 可以去官网上下载自己想要的版本官网可用版本链接 官网 如何去安装lxml模块 下载好这样一个文件:lxml-3.7.2-cp36-cp36m-win32.whl 后在cmd中用 pip install lxml-3.7.2-cp36-cp36m-win32.whl 看提示安装完成后，在cmd中测试查看是否可以使用etree模块,就是进入python,导入etree模块:from lxml import etree,若不报错，说明成功，否则就要检查是哪里出现了问题 注意命名文件时不要和这些模块重名，这样也会导入不成功","categories":[{"name":"Hexo","slug":"Hexo","permalink":"http://yoursite.com/categories/Hexo/"},{"name":"python爬虫","slug":"Hexo/python爬虫","permalink":"http://yoursite.com/categories/Hexo/python爬虫/"}],"tags":[{"name":"Hexo","slug":"Hexo","permalink":"http://yoursite.com/tags/Hexo/"},{"name":"python爬虫","slug":"python爬虫","permalink":"http://yoursite.com/tags/python爬虫/"}]},{"title":"Hello World","slug":"hello-world","date":"2018-09-30T16:00:00.000Z","updated":"2019-03-09T03:53:34.973Z","comments":true,"path":"2018/10/01/hello-world/","link":"","permalink":"http://yoursite.com/2018/10/01/hello-world/","excerpt":"","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new \"My New Post\" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","categories":[{"name":"Hexo","slug":"Hexo","permalink":"http://yoursite.com/categories/Hexo/"}],"tags":[{"name":"Hexo","slug":"Hexo","permalink":"http://yoursite.com/tags/Hexo/"}]}]}