{"meta":{"title":"һ���ĸ���Blog","subtitle":null,"description":null,"author":"bleachһ��","url":"http://yoursite.com","root":"/"},"pages":[{"title":"about","date":"2019-03-07T15:37:41.000Z","updated":"2019-03-07T15:39:04.293Z","comments":false,"path":"about/index.html","permalink":"http://yoursite.com/about/index.html","excerpt":"","text":""}],"posts":[{"title":"Hello World","slug":"hello-world","date":"2019-03-07T13:08:32.147Z","updated":"2019-03-07T13:08:32.148Z","comments":true,"path":"2019/03/07/hello-world/","link":"","permalink":"http://yoursite.com/2019/03/07/hello-world/","excerpt":"","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new \"My New Post\" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","categories":[],"tags":[]},{"title":"","slug":"用python中的lxml模块爬取起点网免费阅读列表","date":"2019-02-25T12:09:20.502Z","updated":"2019-03-04T13:15:53.893Z","comments":true,"path":"2019/02/25/用python中的lxml模块爬取起点网免费阅读列表/","link":"","permalink":"http://yoursite.com/2019/02/25/用python中的lxml模块爬取起点网免费阅读列表/","excerpt":"","text":"用lxml与requests模块爬取起点中文网的免费阅读列表爬取的思路: 首先来讲,爬取内容并非是最重要的,我个人认为要先知道思路,然后再去写代码,就会容易很多。 再就来说说，要写这个爬虫的思路: 首先，应该先看看网站的robots.txt文件，来看一下网站上什么内容可以，让我们去爬取,这里附上其内容，可以看到起点是应该全都可让我们去爬的，然后放心的去写代码。 1234 User-Agent: *Allow: /Allow: /*.cssAllow: /*.js 接下来，就是要获得其URL资源，注意到起点网上的free区的URL是如下的，然后我们尝试着去删除一些无用的信息，来简化URL,来获得同样的页面。https://www.qidian.com/free/all?orderId=&amp;vip=hidden&amp;style=1&amp;pageSize=20&amp;siteid=1&amp;pubflag=0&amp;hiddenField=1&amp;page=2通过尝试我们只要留下 all?page=2 这一部分就可以获得我们想要的结果。于是修改URL为https://www.qidian.com/free/all?page={}这样可以通过format() 方法来构造我们要爬取多少页。 下一步，就要去构造xpath，来获得数据,在chrome浏览器中按F12来查看网页的源码，接着我们就要去找到信息的位置: 12&lt;div class=&quot;book-mid-info&quot;&gt; &lt;h4&gt;&lt;a href=&quot;//book.qidian.com/info/1010741811&quot; target=&quot;_blank&quot; data-eid=&quot;qd_E05&quot; data-bid=&quot;1010741811&quot;&gt;大劫主&lt;/a&gt;&lt;/h4&gt; 对于此xml的节点，我们要得到一个book的名字就只要用etree下的xpath方法，就可以得到想要的信息。代码如下：xpath(&quot;//div[@class=&#39;book-img-text&#39;]//h4//a/text()&quot;)这样就可以得到我们想要的数据了。但是有的地方可能有”\\r”或者是空格只要用replace()方法替代一下就可以了。 最后附上全部的代码：附:爬取的结果图：这样就可以基本上爬到想要的信息了123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354import requestsimport jsonfrom lxml import etreeclass qidian: def __init__(self): self.headers =&#123;&quot;User-Agent&quot;: &quot;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/66.0.3359.139 Safari/537.36&quot;&#125; self.start_url=&quot;https://www.qidian.com/free/all?page=&#123;&#125;&quot; def get_free_url_List(self): return [self.start_url.format(i) for i in range(1,120)] def parse_url(self,url): response=requests.get(url,headers=self.headers) return response.content def get_url_list(self,html_url): html=etree.HTML(html_url) div_lists=html.xpath(&quot;//div[@class=&apos;all-book-list&apos;]&quot;) content_list=[] for div in div_lists: item=&#123;&#125; item[&quot;title&quot;]=div.xpath(&quot;.//h4/a/text()&quot;) item[&quot;title&quot;]=item[&quot;title&quot;][0] if len(item[&quot;title&quot;][0])&gt;0 else None item[&quot;author&quot;]=div.xpath(&quot;.//p[@class=&apos;author&apos;]//a[@class=&apos;name&apos;]/text()&quot;) item[&quot;author&quot;]=item[&quot;author&quot;][0] if len(item[&quot;author&quot;][0])&gt;0 else None item[&quot;content-sum&quot;]=div.xpath(&quot;.//p[@class=&apos;intro&apos;]/text()&quot;) item[&quot;content-sum&quot;]=[div.replace(&quot;\\r&quot;,&quot;&quot;) for div in item[&quot;content-sum&quot;]] item[&quot;content-sum&quot;]=[div.replace(&quot; &quot;,&quot;&quot;) for div in item[&quot;content-sum&quot;]] item[&quot;href&quot;]=&quot;https:&quot;+div.xpath(&quot;.//h4/a/@href&quot;)[0] if len(div.xpath(&quot;.//h4/a/@href&quot;)[0])&gt;0 else None item[&quot;img&quot;]=&quot;https:&quot;+div.xpath(&quot;.//div[@class=&apos;book-img-box&apos;]//img/@src&quot;)[0] if len(div.xpath(&quot;.//div[@class=&apos;book-img-box&apos;]//img/@src&quot;)[0])&gt;0 else None content_list.append(item) return content_list def save_content(self,content_list): with open(&quot;起点中文网免费文章1.txt&quot;,&quot;a&quot;,encoding=&apos;utf-8&apos; ) as f: for i in content_list: f.write(json.dumps(i, ensure_ascii=False)) f.write(&quot;\\n&quot;) # 写入换行符，进行换行 print(&quot;保存成功！&quot;) def run(self): # 发送请求，获得响应 # 接收数据，并处理数据 # 保存数据 url_list=self.get_free_url_List() for url in url_list: html_url=self.parse_url(url) content_list=self.get_url_list(html_url) self.save_content(content_list)if __name__ == &quot;__main__&quot;: spider_qidian=qidian() spider_qidian.run()","categories":[],"tags":[]}]}